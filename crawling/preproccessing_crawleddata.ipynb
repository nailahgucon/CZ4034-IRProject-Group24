{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CZ4034 Information Retrieval Project\n",
        "### Crawling Additional Steps: Preprocessing Code for Crawled Corpus\n",
        "###### Note: Please select \"Runtime\" > \"Run All\" in the navigation bar when running this file in **Google Colab** to ensure smooth execution."
      ],
      "metadata": {
        "id": "EsMzAQ78JXie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Read the original crawled corpus file, reviews_combined.csv, and create a dataframe.\n",
        "\n",
        "  Note: Upload the reviews_combined.csv file to Google Colab using the \"Upload to Session Storage\" button on the left."
      ],
      "metadata": {
        "id": "WOBsKO3_Jgpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_og = pd.read_csv('reviews_combined.csv')"
      ],
      "metadata": {
        "id": "U2CJ32c74BBJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Import and download necessary libraries for tokenization and lemmatization."
      ],
      "metadata": {
        "id": "2UjEDId7KEwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# for tokenization\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPcYpaAw4k5I",
        "outputId": "8208360b-d984-458e-ef2a-db79f2559f20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for lemmatization\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZiHpKRV4ykc",
        "outputId": "5b0fdd8d-48fb-443a-af9e-aedd399338df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Apply the following preprocessing steps on the `Review` column: \n",
        "Conversion to lowercase, decoding of HTML entities, removal of Remove non-ASCII characters, removal of punctuation, tokenization, removal of a custom set of stopwords, remove hashtags, lemmatization."
      ],
      "metadata": {
        "id": "L5kLbU5gLPCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import re\n",
        "import html\n",
        "\n",
        "# load the CSV file\n",
        "df = pd.read_csv('reviews_combined.csv')\n",
        "\n",
        "# define a function to perform the pre-processing steps\n",
        "def preprocess_text(text):\n",
        "    # convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Decode HTML entities\n",
        "    text = html.unescape(text)\n",
        "\n",
        "    # Remove non-ASCII characters\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "    \n",
        "    # remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Replace \" t \" with \"t \" to combine words like \"couldn t \" after removal of punctuation\n",
        "    text = text.replace(\" t \", \"t \")\n",
        "    \n",
        "    # tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # remove custom set of stopwords\n",
        "    stop_words = set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'if', 'or', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'nor', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ma'])\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    \n",
        "    # remove hashtags\n",
        "    filtered_tokens = [token for token in filtered_tokens if not token.startswith('#')]\n",
        "    \n",
        "    # lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    filtered_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "    \n",
        "    # join the filtered tokens into a string\n",
        "    text = ' '.join(filtered_tokens)\n",
        "    \n",
        "    return text\n",
        "\n",
        "# apply the pre-processing function to the review column in the DataFrame\n",
        "df['Review'] = df['Review'].apply(preprocess_text)\n",
        "\n",
        "# output the file as preprocessed_reviews_combined.csv\n",
        "df.to_csv('preprocessed_reviews_combined.csv', index=False)"
      ],
      "metadata": {
        "id": "d-ZlC7o430zI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Find the total number of words in column `Review` before and after preprocessing (it will be the same)."
      ],
      "metadata": {
        "id": "KbrTkiahL362"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV file into a Pandas dataframe\n",
        "df = pd.read_csv('reviews_combined.csv')\n",
        "\n",
        "# Get the number of rows in the dataframe\n",
        "num_rows = len(df)\n",
        "\n",
        "# Print the total number of records\n",
        "print('Total number of records:', num_rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT1b8hb8r_ub",
        "outputId": "bce0595a-f1a4-43a2-8ecf-1f4e927fc4b0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of records: 17669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Find the total number of words in column `Review` before and after preprocessing."
      ],
      "metadata": {
        "id": "dPVOTVLJLkK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset into a DataFrame\n",
        "df = pd.read_csv('reviews_combined.csv')\n",
        "df2 = pd.read_csv('preprocessed_reviews_combined.csv')\n",
        "\n",
        "# Define the column that contains the text\n",
        "text_column = 'Review'\n",
        "\n",
        "# Concatenate all the texts in the column into a single string\n",
        "all_text = ' '.join(df[text_column].tolist())\n",
        "all_text2 = ' '.join(df2[text_column].tolist())\n",
        "\n",
        "# Split the text into words\n",
        "words = all_text.split()\n",
        "words2 = all_text2.split()\n",
        "\n",
        "# Count the number of words\n",
        "num_words = len(words)\n",
        "num_words2 = len(words2)\n",
        "\n",
        "# Print the total number of words in column Review before and after preprocessing\n",
        "print(f'Total number of words in column \"{text_column}\" (before preprocessing): {num_words}')\n",
        "print(f'Total number of words in column \"{text_column}\" (after preprocessing): {num_words2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpcEpjUGntwQ",
        "outputId": "35681ce8-697e-4f57-d120-d708402af388"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of words in column \"Review\" (before preprocessing): 1281072\n",
            "Total number of words in column \"Review\" (after preprocessing): 715643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Find the total number of unique words in column `Review` before and after preprocessing."
      ],
      "metadata": {
        "id": "Dwj5ZuOoLwT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data as a pandas dataframe\n",
        "df = pd.read_csv(\"reviews_combined.csv\")\n",
        "df2 = pd.read_csv(\"preprocessed_reviews_combined.csv\")\n",
        "\n",
        "# Define the column that we want to analyze\n",
        "column_to_analyze = \"Review\"\n",
        "\n",
        "# Concatenate all the reviews into a single string\n",
        "all_reviews = \" \".join(df[column_to_analyze].tolist())\n",
        "all_reviews2 = \" \".join(df2[column_to_analyze].tolist())\n",
        "\n",
        "# Split the string into words\n",
        "words = all_reviews.split()\n",
        "words2 = all_reviews2.split()\n",
        "\n",
        "# Count the unique words\n",
        "unique_words = set(words)\n",
        "num_unique_words = len(unique_words)\n",
        "unique_words2 = set(words2)\n",
        "num_unique_words2 = len(unique_words2)\n",
        "\n",
        "# Print the total number of unique words in column Review before and after preprocessing\n",
        "print(\"Number of unique words in the column '{}' (before preprocessing): {}\".format(column_to_analyze, num_unique_words))\n",
        "print(\"Number of unique words in the column '{}' (after preprocessing): {}\".format(column_to_analyze, num_unique_words2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91PhVJNyp19l",
        "outputId": "9fde2798-2b72-41d9-f46c-c8489b087602"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in the column 'Review' (before preprocessing): 62475\n",
            "Number of unique words in the column 'Review' (after preprocessing): 26760\n"
          ]
        }
      ]
    }
  ]
}